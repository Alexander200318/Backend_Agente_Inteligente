# app/ollama/prompt_builder.py
from pathlib import Path
from models.agente_virtual import AgenteVirtual

#  Usar ruta absoluta
BASE_DIR = Path(__file__).parent.parent
TEMPLATES_DIR = BASE_DIR / "templates"

def build_system_prompt(agente: AgenteVirtual) -> str:
    """
    Construye prompt del sistema usando datos reales del agente
    """
    # Opci贸n 1: Usar prompt_sistema directamente si existe
    if agente.prompt_sistema:
        return agente.prompt_sistema
    
    # Opci贸n 2: Construir din谩micamente
    prompt = f"""Eres {agente.nombre_agente}.

**Tu especialidad:** {agente.area_especialidad or 'Asistente general'}

**Descripci贸n:** {agente.descripcion or 'Asistente virtual'}

**Instrucciones adicionales:**
{agente.prompt_especializado or ''}

**Reglas importantes:**
- NO inventes informaci贸n que no est茅 en el CONTEXTO proporcionado
- Si no sabes algo, dilo honestamente
- Responde de forma clara y concisa
- Si el CONTEXTO est谩 vac铆o, indica que no tienes informaci贸n suficiente
"""
    return prompt.strip()

def build_chat_prompt(system_prompt: str, contexto: str, pregunta: str) -> str:
    """
    Construye prompt final para el chat
    """
    # Verificar si existe el template, si no usar inline
    template_path = TEMPLATES_DIR / "chat_prompt_template.txt"
    
    if template_path.exists():
        tpl = template_path.read_text(encoding="utf-8")
        return tpl.format(
            system=system_prompt, 
            contexto=contexto, 
            pregunta=pregunta
        )
    
    # Fallback: template inline
    return f"""{system_prompt}

---

**CONTEXTO RELEVANTE:**
{contexto if contexto else "No se encontr贸 informaci贸n espec铆fica."}

---

**PREGUNTA DEL USUARIO:**
{pregunta}

---

**TU RESPUESTA:**"""